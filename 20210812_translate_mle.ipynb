{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7beaa5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n",
      "Version:  2.5.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.12.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f5fef",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5582366",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = '20210812_wmt19_cs_en'\n",
    "folder_name = '20210812_translate_mle'\n",
    "\n",
    "wv_dim = 32\n",
    "en_que_pad = 30\n",
    "de_que_pad = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34426dc1",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9d9bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_train = pickle.load(open(f'{d_name}/encoder_train.pkl', 'rb'))\n",
    "decoder_train = pickle.load(open(f'{d_name}/decoder_train.pkl', 'rb'))\n",
    "teacher_train = pickle.load(open(f'{d_name}/teacher_train.pkl', 'rb'))\n",
    "encoder_vali  = pickle.load(open(f'{d_name}/encoder_vali.pkl', 'rb'))\n",
    "decoder_vali  = pickle.load(open(f'{d_name}/decoder_vali.pkl', 'rb'))\n",
    "teacher_vali  = pickle.load(open(f'{d_name}/teacher_vali.pkl', 'rb'))\n",
    "\n",
    "decoder_idx2word   = pickle.load(open(f'{d_name}/en_idx2word.pkl','rb'))\n",
    "decoder_word2idx   = pickle.load(open(f'{d_name}/en_word2idx.pkl','rb'))\n",
    "encoder_idx2word   = pickle.load(open(f'{d_name}/cs_idx2word.pkl','rb'))\n",
    "encoder_word2idx   = pickle.load(open(f'{d_name}/cs_word2idx.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c42e44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52575\n",
      "52573\n"
     ]
    }
   ],
   "source": [
    "num_decoder_words = np.max([np.max(decoder_train), np.max(decoder_vali)])+1\n",
    "num_encoder_words = np.max([np.max(encoder_train), np.max(encoder_vali)])+1\n",
    "\n",
    "print(num_decoder_words)\n",
    "print(num_encoder_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a9e43ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2word(seq_tensor, idx2word):\n",
    "    return np.array([[idx2word[str(i)] for i in seq] for seq in seq_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e01791e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['bos', '10', '000', 'gold', 'eos', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad'],\n",
       "       ['bos', 'san', 'francisco', '–', 'it', 'has', 'never', 'been',\n",
       "        'easy', 'to', 'have', 'a', 'rational', 'conversation', 'about',\n",
       "        'the', 'value', 'of', 'gold', 'eos', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad'],\n",
       "       ['bos', 'lately', 'with', 'gold', 'prices', 'up', 'more', 'than',\n",
       "        '300', 'over', 'the', 'last', 'decade', 'it', 'is', 'harder',\n",
       "        'than', 'ever', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad'],\n",
       "       ['bos', 'just', 'last', 'december', 'fellow', 'economists',\n",
       "        'martin', 'feldstein', 'and', 'nouriel', 'roubini', 'each',\n",
       "        'penned', 'op', 'eds', 'bravely', 'questioning', 'bullish',\n",
       "        'market', 'sentiment', 'sensibly', 'pointing', 'out', 'gold’s',\n",
       "        'risks', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad'],\n",
       "       ['bos', 'wouldn’t', 'you', 'know', 'it', 'eos', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad', 'pad'],\n",
       "       ['bos', 'since', 'their', 'articles', 'appeared', 'the', 'price',\n",
       "        'of', 'gold', 'has', 'moved', 'up', 'still', 'further', 'gold',\n",
       "        'prices', 'even', 'hit', 'a', 'record', 'high', '1', '300',\n",
       "        'recently', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad'],\n",
       "       ['bos', 'last', 'december', 'many', 'gold', 'bugs', 'were',\n",
       "        'arguing', 'that', 'the', 'price', 'was', 'inevitably', 'headed',\n",
       "        'for', '2', '000', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad'],\n",
       "       ['bos', 'now', 'emboldened', 'by', 'continuing', 'appreciation',\n",
       "        'some', 'are', 'suggesting', 'that', 'gold', 'could', 'be',\n",
       "        'headed', 'even', 'higher', 'than', 'that', 'eos', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad', 'pad'],\n",
       "       ['bos', 'since', 'then', 'the', 'index', 'has', 'climbed',\n",
       "        'above', '10', '000', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad'],\n",
       "       ['bos', 'now', 'that', 'gold', 'has', 'crossed', 'the', 'magic',\n",
       "        '1', '000', 'barrier', 'why', 'can’t', 'it', 'increase', 'ten',\n",
       "        'fold', 'too', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad',\n",
       "        'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']], dtype='<U12')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2word(decoder_vali[:10], decoder_idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e88a4f1",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116afbb5",
   "metadata": {},
   "source": [
    "[OneHot relaxation](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/RelaxedOneHotCategorical)\n",
    "based on [Jang+16](https://arxiv.org/abs/1611.01144) and [Maddison+16](https://arxiv.org/abs/1611.00712) \n",
    "<br>\n",
    "[Multi-Head Attention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) \n",
    "and [Positional Encoding](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "based on [Vaswani+17, Attention is All You Need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff252096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getG():\n",
    "    \n",
    "    # Encoder\n",
    "    en_input = Input((en_que_pad,))\n",
    "    en_emb = Embedding(\n",
    "        num_encoder_words, \n",
    "        wv_dim, \n",
    "        mask_zero = False,\n",
    "        input_length = en_que_pad,\n",
    "        trainable = True,\n",
    "    )\n",
    "    vector_encoder = en_emb(en_input)\n",
    "    \n",
    "    # Deocder\n",
    "    de_input = Input((de_que_pad,))\n",
    "    de_emb = Embedding(\n",
    "        num_decoder_words, \n",
    "        wv_dim, \n",
    "        mask_zero = False,\n",
    "        input_length = de_que_pad,\n",
    "        trainable = True,\n",
    "    )\n",
    "    vector_decoder = de_emb(de_input)\n",
    "    mem, state_h, state_c = LSTM(wv_dim, return_state=True)(vector_encoder)\n",
    "    output=LSTM(wv_dim, return_sequences=True)(vector_decoder, initial_state=[state_h, state_c] )\n",
    "    # Concatenation and output\n",
    "    output = Dense(num_decoder_words)(output)\n",
    "    output = Activation('softmax')(output)\n",
    "    model = Model(\n",
    "        [en_input, de_input], \n",
    "        output,\n",
    "    ) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed0171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 32)       1682336     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 32)       1682400     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 32), (None,  8320        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32, 32)       8320        embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32, 52575)    1734975     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 52575)    0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 5,116,351\n",
      "Trainable params: 5,116,351\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 16:02:47.834596: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-08-14 16:02:47.834678: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "mleG=getG()\n",
    "mleG.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "mleG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cf56ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 16:02:48.057736: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-08-14 16:02:48.057916: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2021-08-14 16:02:48.826842: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-08-14 16:02:49.007855: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-08-14 16:02:49.159475: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-08-14 16:02:49.628379: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "   1/2716 [..............................] - ETA: 1:30:40 - loss: 10.8699 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 16:02:49.892891: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2716/2716 [==============================] - ETA: 0s - loss: 4.4608 - accuracy: 0.4472"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 16:21:11.742186: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-08-14 16:21:11.830678: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-08-14 16:21:11.995634: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2716/2716 [==============================] - 1192s 438ms/step - loss: 4.4608 - accuracy: 0.4472 - val_loss: 4.0409 - val_accuracy: 0.4479\n",
      "Epoch 2/300\n",
      "2716/2716 [==============================] - 1243s 458ms/step - loss: 3.9941 - accuracy: 0.4657 - val_loss: 3.9332 - val_accuracy: 0.4850\n",
      "Epoch 3/300\n",
      "  11/2716 [..............................] - ETA: 17:52 - loss: 3.9195 - accuracy: 0.4812"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zh/f93mc8qj0gn6jf1xs02wnwk40000gn/T/ipykernel_51637/3063561380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m mleG.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mencoder_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mteacher_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mleG.fit(\n",
    "    [encoder_train, decoder_train], \n",
    "    teacher_train, \n",
    "    batch_size=64, \n",
    "    epochs=300, \n",
    "    shuffle=True, \n",
    "    validation_data = (\n",
    "        [encoder_vali, decoder_vali], \n",
    "        teacher_vali\n",
    "    ), \n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            f'./{folder_name}/mleG.h5', \n",
    "            save_best_only=True, \n",
    "            monitor = \"val_loss\"\n",
    "        ),\n",
    "        EarlyStopping(monitor='val_loss', patience=5),\n",
    "        CSVLogger(f'{folder_name}/mleG.csv'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cdb0a8",
   "metadata": {},
   "source": [
    "## Inference train D (False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd628cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    model,\n",
    "    enData = None,\n",
    "    inpData = None,\n",
    "    start_on = 0,\n",
    "    end_on = 10,\n",
    "    num_data = 10, \n",
    "    batch_size = 10,\n",
    "    que_pad = 10,\n",
    "):\n",
    "    # Initialize\n",
    "    num_batch = (num_data-1)//batch_size +1\n",
    "    resp_pred_list = None\n",
    "    in_batch_list = None\n",
    "    the_first = True\n",
    "    #idx = np.arange(num_words)\n",
    "    for b in range(num_batch):\n",
    "        in_batch = np.zeros((batch_size, que_pad), dtype = int)\n",
    "        if start_on == 0:\n",
    "            in_batch[:,0] = word2idx['BOS']\n",
    "        else:\n",
    "            in_batch = inpData[b*batch_size:(b+1)*batch_size]\n",
    "            en_batch = enData[b*batch_size:(b+1)*batch_size]\n",
    "        resp_pred = np.zeros((batch_size, que_pad), dtype = int)\n",
    "        # Generate the sequence recurrsively.\n",
    "        for i in range(start_on, end_on):\n",
    "            # Run\n",
    "            resp_pred_wv = model([en_batch, in_batch])\n",
    "            the_last = resp_pred_wv[:,i]\n",
    "            #We the_last = tf.keras.backend.argmax(the_last).numpy()\n",
    "            the_last = tf.reshape(\n",
    "                tf.random.categorical(tf.math.log(the_last), 1), \n",
    "                [batch_size,]\n",
    "            )\n",
    "            try:\n",
    "                resp_pred[:,i] = the_last\n",
    "                in_batch[:,i+1] = the_last\n",
    "            except:\n",
    "                resp_pred[:,i] = the_last\n",
    "        for i in range(len(resp_pred)):\n",
    "            try:\n",
    "                index = list(resp_pred[i]).index(word2idx['EOS'])\n",
    "            except:\n",
    "                continue\n",
    "            resp_pred[i,index+1:] = 0\n",
    "            in_batch[i,index+1:] = 0\n",
    "        if the_first:\n",
    "            resp_pred_list = resp_pred\n",
    "            in_batch_list = in_batch\n",
    "            the_first = False\n",
    "        else:\n",
    "            resp_pred_list = np.vstack((resp_pred_list, resp_pred))\n",
    "            in_batch_list = np.vstack((in_batch_list, in_batch))\n",
    "    resp_pred_list = resp_pred_list[:num_data]\n",
    "    in_batch_list = in_batch_list[:num_data]\n",
    "    if start_on != 0:\n",
    "        resp_pred_list[:,:start_on] = inpData[:,1:start_on+1]\n",
    "        in_batch_list[:, :start_on+1] = inpData[:,:start_on+1]\n",
    "    return resp_pred_list, in_batch_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
